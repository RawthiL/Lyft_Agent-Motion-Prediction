{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using: LyftAgent_lib \t branch master \t commit hash df7c2d9ee4ec0471290a5590b9d6274611bc223c\n\t\t WARNING -- modified files:\n['AgentPrediction_Train.ipynb', 'AgentPrediction_config.yaml', 'AgentPrediction_config_Baseline.yaml', 'LyftAgent_lib/train_support.py']\npython: 3.6.9\nUsing TensorFlow version: 2.3.1\nUsing Keras version: 2.4.0\n[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 6266086559033967116\n, name: \"/device:XLA_CPU:0\"\ndevice_type: \"XLA_CPU\"\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 4701137181891413462\nphysical_device_desc: \"device: XLA_CPU device\"\n]\n"
     ]
    }
   ],
   "source": [
    "# matplotlib plots within notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os, sys\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import ChunkedDataset, LocalDataManager\n",
    "from l5kit.dataset import AgentDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv\n",
    "from l5kit.geometry import transform_points\n",
    "\n",
    "\n",
    "# Custom libs\n",
    "sys.path.insert(0, './LyftAgent_lib')\n",
    "from LyftAgent_lib import train_support as lyl_ts\n",
    "from LyftAgent_lib import topologies as lyl_nn\n",
    "\n",
    "# Print Code Version\n",
    "import git\n",
    "def print_git_info(path, nombre):\n",
    "    repo = git.Repo(path)\n",
    "    print('Using: %s \\t branch %s \\t commit hash %s'%(nombre, repo.active_branch.name, repo.head.object.hexsha))\n",
    "    changed = [ item.a_path for item in repo.index.diff(None) ]\n",
    "    if len(changed)>0:\n",
    "        print('\\t\\t WARNING -- modified files:')\n",
    "        print(changed)\n",
    "    \n",
    "print_git_info('.', 'LyftAgent_lib')\n",
    "\n",
    "\n",
    "import platform\n",
    "print(\"python: \"+platform.python_version())\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "print('Using TensorFlow version: '+tf.__version__)\n",
    "print('Using Keras version: '+keras.__version__)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model loction\n",
    "path_load = ''\n",
    "# Test model base name\n",
    "net_base_name = ''\n",
    "\n",
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"\"\n",
    "# get config\n",
    "cfg = load_config_data(\"./AgentPrediction_config.yaml\")\n",
    "cfg = lyl_ts.fill_defaults(cfg)\n",
    "\n",
    "# Validation chopped dataset:\n",
    "eval_base_path = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_map_input_shape      = (cfg[\"raster_params\"][\"raster_size\"][0],\n",
    "                              cfg[\"raster_params\"][\"raster_size\"][1])\n",
    "num_hist_frames            = cfg[\"model_params\"][\"history_num_frames\"]\n",
    "num_future_frames          = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "\n",
    "increment_net              = cfg[\"model_params\"][\"increment_net\"]\n",
    "\n",
    "mruv_guiding               = cfg[\"model_params\"][\"mruv_guiding\"]\n",
    "mruv_model_trainable       = cfg[\"model_params\"][\"mruv_model_trainable\"]\n",
    "\n",
    "retrain_inputs_image_model = cfg[\"training_params\"][\"retrain_inputs_image_model\"]\n",
    "gen_batch_size             = cfg[\"train_data_loader\"][\"batch_size\"]\n",
    "\n",
    "\n",
    "model_version              = cfg[\"model_params\"][\"version\"]\n",
    "base_image_preprocess      = cfg[\"model_params\"][\"base_image_preprocess\"]\n",
    "use_fading                 = cfg[\"model_params\"][\"use_fading\"]\n",
    "\n",
    "use_angle                  = cfg[\"model_params\"][\"use_angle\"]\n",
    "\n",
    "isBaseModel = False\n",
    "if model_version == 'Base':\n",
    "    isBaseModel = True\n",
    "    forward_pass_use = lyl_nn.modelBaseline_forward_pass\n",
    "elif model_version == 'V1':\n",
    "    forward_pass_use = lyl_nn.modelV1_forward_pass\n",
    "elif model_version == 'V2':\n",
    "    forward_pass_use = lyl_nn.modelV2_forward_pass\n",
    "    \n",
    "\n",
    "# Get image preprocessing function (depends on image encoding base architecture)\n",
    "if base_image_preprocess == None:\n",
    "    base_image_preprocess_fcn = lambda x: x\n",
    "else:\n",
    "    try:\n",
    "        base_image_preprocess_fcn = getattr(keras.applications, base_image_preprocess.split('.')[0])\n",
    "        base_image_preprocess_fcn = getattr(base_image_preprocess_fcn, base_image_preprocess.split('.')[1])\n",
    "    except:\n",
    "        raise Exception('Base image pre-processing not found. Requested function: %s'%base_image_preprocess) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "All models succesfully loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_list = lyl_ts.load_models(path_load, net_base_name, \n",
    "                         load_img_model = (retrain_inputs_image_model or retrain_all_image_model), \n",
    "                         isBaseModel = isBaseModel,\n",
    "                         mruv_guiding = mruv_guiding)\n",
    "\n",
    "ImageEncModel = model_list[0]\n",
    "HistEncModel = model_list[1]\n",
    "PathDecModel = model_list[2]\n",
    "mruv_model = model_list[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   11314    |  1131400   |  88594921  |    7854144    |      31.43      |        100.00        |        78.31         |        10.00         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: you're running with a custom agents_mask\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "dm = LocalDataManager()\n",
    "rast = build_rasterizer(cfg, dm)\n",
    "\n",
    "dataset_path_test = dm.require(cfg[\"test_data_loader\"][\"key\"])\n",
    "test_zarr = ChunkedDataset(dataset_path_test)\n",
    "test_zarr.open()\n",
    "print(test_zarr)\n",
    "\n",
    "test_mask = np.load(f\"../prediction-dataset/scenes/mask.npz\")[\"arr_0\"]\n",
    "\n",
    "test_dataset = AgentDataset(cfg, test_zarr, rast, agents_mask=test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating dataset with: \n\t Randomized scenes: False\n\t Randomized frames: False\n\t Number of scenes: 11314\n\t Number of frames per scenes: -1\n"
     ]
    }
   ],
   "source": [
    "tf_test_dataset = lyl_ts.get_tf_dataset(test_dataset, \n",
    "                                             num_hist_frames,\n",
    "                                             model_map_input_shape,\n",
    "                                             num_future_frames,\n",
    "                                             meta_dict_use = lyl_ts.meta_dict_pass)\n",
    "\n",
    "# Map sample pre-processing function\n",
    "tf_test_dataset = tf_test_dataset.map(lambda x: lyl_ts.tf_get_input_sample(x, \n",
    "                                                                             image_preprocess_fcn=base_image_preprocess_fcn, \n",
    "                                                                             use_fading = use_fading,\n",
    "                                                                             use_angle = use_angle))\n",
    "# Set batch size\n",
    "tf_test_dataset = tf_test_dataset.batch(batch_size=gen_batch_size)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing: :  19%|█▉        | 417/2223 [13:25<58:06,  1.93s/it]  \n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-327198561c79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m for (thisSampleMapComp, thisSampeHistPath, thisSampeTargetPath, \n\u001b[1;32m     10\u001b[0m     \u001b[0mthisHistAvail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthisTargetAvail\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     thisTimeStamp, thisTrackID, thisRasterFromAgent, thisWorldFromAgent, thisCentroid, thisSampleIdx) in test_dataset_prog_bar:\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpad_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_batch_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mthisSampleMapComp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    770\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2605\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2607\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2608\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2609\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "future_coords_offsets_pd = []\n",
    "timestamps = []\n",
    "agent_ids = []\n",
    "\n",
    "\n",
    "\n",
    "test_dataset_prog_bar = tqdm(tf_test_dataset, total=int(np.ceil(len(test_dataset)/gen_batch_size)))\n",
    "test_dataset_prog_bar.set_description('Testing: ')\n",
    "for (thisSampleMapComp, thisSampeHistPath, thisSampeTargetPath, \n",
    "    thisHistAvail, thisTargetAvail, \n",
    "    thisTimeStamp, thisTrackID, thisRasterFromAgent, thisWorldFromAgent, thisCentroid, thisSampleIdx) in test_dataset_prog_bar:\n",
    "    \n",
    "    pad_size = gen_batch_size-thisSampleMapComp.shape[0]\n",
    "    if pad_size != 0:\n",
    "        # Create padded batch if necessary\n",
    "        aux_SampleMapComp = np.zeros((gen_batch_size,thisSampleMapComp.shape[1],thisSampleMapComp.shape[2],thisSampleMapComp.shape[3]), dtype = np.float32)\n",
    "        aux_SampeHistPath = np.zeros((gen_batch_size,thisSampeHistPath.shape[1],thisSampeHistPath.shape[2]), dtype = np.float32)\n",
    "        aux_SampeTargetPath = np.zeros((gen_batch_size,thisSampeTargetPath.shape[1],thisSampeTargetPath.shape[2]), dtype = np.float32)\n",
    "        aux_HistAvail = np.zeros((gen_batch_size,thisHistAvail.shape[1]), dtype = np.float32)\n",
    "        aux_TargetAvail = np.zeros((gen_batch_size,thisTargetAvail.shape[1]), dtype = np.float32)\n",
    "        aux_TimeStamp = np.zeros((gen_batch_size), dtype = np.float32)\n",
    "        aux_TrackID = np.zeros((gen_batch_size), dtype = np.float32)\n",
    "\n",
    "        aux_SampleMapComp[:gen_batch_size-pad_size,:,:,:] = thisSampleMapComp\n",
    "        aux_SampeHistPath[:gen_batch_size-pad_size,:,:] = thisSampeHistPath\n",
    "        aux_SampeTargetPath[:gen_batch_size-pad_size,:,:] = thisSampeTargetPath\n",
    "        aux_HistAvail[:gen_batch_size-pad_size,:] = thisHistAvail\n",
    "        aux_TargetAvail[:gen_batch_size-pad_size,:] = thisTargetAvail\n",
    "        aux_TimeStamp[:gen_batch_size-pad_size] = thisTimeStamp\n",
    "        aux_TrackID[:gen_batch_size-pad_size] = thisTrackID\n",
    "\n",
    "        thisSampleMapComp = aux_SampleMapComp\n",
    "        thisSampeHistPath = aux_SampeHistPath\n",
    "        thisSampeTargetPath = aux_SampeTargetPath\n",
    "        thisHistAvail = aux_HistAvail\n",
    "        thisTargetAvail = aux_TargetAvail\n",
    "        TimeStamp = aux_TimeStamp\n",
    "        thisTrackID = aux_TrackID\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Predict\n",
    "    if base_model:\n",
    "        predPath = forwardpass_use(thisSampleMapComp, ImageEncModel,  PathDecModel)\n",
    "    else:\n",
    "        PathDecModel.reset_states()\n",
    "        HistEncModel.reset_states()\n",
    "        predPath = forwardpass_use(thisSampleMapComp, thisSampeHistPath, thisHistAvail, 50,\n",
    "                                                ImageEncModel, HistEncModel, PathDecModel,\n",
    "                                                use_teacher_force=False,\n",
    "                                                increment_net = increment_net,\n",
    "                                                mruv_guiding = mruv_guiding,\n",
    "                                                mruv_model = mruv_model,\n",
    "                                                mruv_model_trainable = mruv_model_trainable)\n",
    "\n",
    "    predPath = predPath.numpy()\n",
    "\n",
    "    # convert agent coordinates into world offsets\n",
    "    predPath = predPath[:,:,:2]\n",
    "    world_from_agents = thisWorldFromAgent.numpy()\n",
    "    centroids = thisCentroid.numpy()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for idx_sample in range(gen_batch_size-pad_size):\n",
    "        \n",
    "        # Save info\n",
    "        future_coords_offsets_pd.append(transform_points(predPath[idx_sample,:,:], thisWorldFromAgent.numpy()[idx_sample,:,:]) -thisCentroid.numpy()[idx_sample,:])\n",
    "        timestamps.append(thisTimeStamp[idx_sample].numpy())\n",
    "        agent_ids.append(thisTrackID[idx_sample])\n",
    "\n",
    "assert len(agent_ids) == len(test_dataset), \"Test data size not equal to dataset.\"\n",
    "    "
   ]
  },
  {
   "source": [
    "### Write result csv\n",
    "\n",
    "    Encode the predictions into a csv file. Coords can have an additional axis for multi-mode.\n",
    "    We handle up to MAX_MODES modes.\n",
    "    For the uni-modal case (i.e. all predictions have just a single mode), coords should not have the additional axis\n",
    "    and confs should be set to None. In this case, a single mode with confidence 1 will be written.\n",
    "    Args:\n",
    "        csv_path (str): path to the csv to write\n",
    "        timestamps (np.ndarray): (num_example,) frame timestamps\n",
    "        track_ids (np.ndarray): (num_example,) agent ids\n",
    "        coords (np.ndarray): (num_example x (modes) x future_len x num_coords) meters displacements\n",
    "        confs (Optional[np.ndarray]): (num_example x modes) confidence of each modes in each example.\n",
    "        Rows should sum to 1\n",
    "    Returns:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a98f509704e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m write_pred_csv('submission.csv',\n\u001b[0;32m----> 2\u001b[0;31m                \u001b[0mtimestamps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                \u001b[0mtrack_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                coords=np.concatenate(future_coords_offsets_pd))\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "write_pred_csv('submission.csv',\n",
    "               timestamps=np.array(timestamps),\n",
    "               track_ids=np.array(agent_ids),\n",
    "               coords=np.array(future_coords_offsets_pd),\n",
    "              confs=None)"
   ]
  },
  {
   "source": [
    "# Validation \n",
    "\n",
    "This will perform validation against a chopped dataset. It will provide a value near the test value of the leader board."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace, rmse, average_displacement_error_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_val = LocalDataManager()\n",
    "\n",
    "# ===== GENERATE AND LOAD CHOPPED DATASET\n",
    "num_frames_to_chop = 100\n",
    "eval_cfg = cfg[\"val_data_loader\"]\n",
    "if not os.path.exists(eval_base_path):\n",
    "    eval_base_path = create_chopped_dataset(dm_val.require(eval_cfg[\"key\"]), \n",
    "                                            cfg[\"raster_params\"][\"filter_agents_threshold\"], \n",
    "                                            num_frames_to_chop,\n",
    "                                            cfg[\"model_params\"][\"future_num_frames\"],\n",
    "                                            MIN_FUTURE_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: you're running with a custom agents_mask\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   16220    |  1622000   | 125423254  |    11733321   |      45.06      |        100.00        |        77.33         |        10.00         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "rast_val = build_rasterizer(cfg, dm_val)\n",
    "\n",
    "\n",
    "eval_zarr_path = os.path.join(eval_base_path, 'validate.zarr')\n",
    "eval_mask_path = os.path.join(eval_base_path, 'mask.npz') \n",
    "eval_gt_path =  os.path.join(eval_base_path, 'gt.csv') \n",
    "\n",
    "eval_zarr = ChunkedDataset(eval_zarr_path).open()\n",
    "eval_mask = np.load(eval_mask_path)[\"arr_0\"]\n",
    "# ===== INIT DATASET AND LOAD MASK\n",
    "validation_dataset = AgentDataset(cfg, eval_zarr, rast_val, agents_mask=eval_mask)\n",
    "# eval_dataloader = DataLoader(eval_dataset, shuffle=eval_cfg[\"shuffle\"], batch_size=eval_cfg[\"batch_size\"], \n",
    "#                              num_workers=eval_cfg[\"num_workers\"])\n",
    "print(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating dataset with: \n\t Randomized scenes: False\n\t Randomized frames: False\n\t Number of scenes: 16220\n\t Number of frames per scenes: -1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf_validation_dataset = lyl_ts.get_tf_dataset(validation_dataset, \n",
    "                                             num_hist_frames,\n",
    "                                             model_map_input_shape,\n",
    "                                             num_future_frames,\n",
    "                                             meta_dict_use = lyl_ts.meta_dict_pass)\n",
    "\n",
    "# Map sample pre-processing function\n",
    "tf_validation_dataset = tf_validation_dataset.map(lambda x: lyl_ts.tf_get_input_sample(x, \n",
    "                                                                             image_preprocess_fcn=base_image_preprocess_fcn, \n",
    "                                                                             use_fading = use_fading,\n",
    "                                                                             use_angle = use_angle))\n",
    "# Set batch size\n",
    "tf_validation_dataset = tf_validation_dataset.batch(batch_size=gen_batch_size)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating: : 100%|██████████| 2960/2960 [1:35:32<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "future_coords_offsets_pd = []\n",
    "timestamps = []\n",
    "agent_ids = []\n",
    "\n",
    "val_dataset_prog_bar = tqdm(tf_validation_dataset, total=int(np.ceil(len(validation_dataset)/gen_batch_size)))\n",
    "val_dataset_prog_bar.set_description('Validating: ')\n",
    "for (thisSampleMapComp, thisSampeHistPath, thisSampeTargetPath, \n",
    "    thisHistAvail, thisTargetAvail, \n",
    "    thisTimeStamp, thisTrackID, thisRasterFromAgent, thisWorldFromAgent, thisCentroid, thisSampleIdx) in val_dataset_prog_bar:\n",
    "    \n",
    "    pad_size = gen_batch_size-thisSampleMapComp.shape[0]\n",
    "    if pad_size != 0:\n",
    "        # Create padded batch if necessary\n",
    "        aux_SampleMapComp = np.zeros((gen_batch_size,thisSampleMapComp.shape[1],thisSampleMapComp.shape[2],thisSampleMapComp.shape[3]), dtype = np.float32)\n",
    "        aux_SampeHistPath = np.zeros((gen_batch_size,thisSampeHistPath.shape[1],thisSampeHistPath.shape[2]), dtype = np.float32)\n",
    "        aux_SampeTargetPath = np.zeros((gen_batch_size,thisSampeTargetPath.shape[1],thisSampeTargetPath.shape[2]), dtype = np.float32)\n",
    "        aux_HistAvail = np.zeros((gen_batch_size,thisHistAvail.shape[1]), dtype = np.float32)\n",
    "        aux_TargetAvail = np.zeros((gen_batch_size,thisTargetAvail.shape[1]), dtype = np.float32)\n",
    "        aux_TimeStamp = np.zeros((gen_batch_size), dtype = np.float32)\n",
    "        aux_TrackID = np.zeros((gen_batch_size), dtype = np.float32)\n",
    "\n",
    "        aux_SampleMapComp[:gen_batch_size-pad_size,:,:,:] = thisSampleMapComp\n",
    "        aux_SampeHistPath[:gen_batch_size-pad_size,:,:] = thisSampeHistPath\n",
    "        aux_SampeTargetPath[:gen_batch_size-pad_size,:,:] = thisSampeTargetPath\n",
    "        aux_HistAvail[:gen_batch_size-pad_size,:] = thisHistAvail\n",
    "        aux_TargetAvail[:gen_batch_size-pad_size,:] = thisTargetAvail\n",
    "        aux_TimeStamp[:gen_batch_size-pad_size] = thisTimeStamp\n",
    "        aux_TrackID[:gen_batch_size-pad_size] = thisTrackID\n",
    "\n",
    "        thisSampleMapComp = aux_SampleMapComp\n",
    "        thisSampeHistPath = aux_SampeHistPath\n",
    "        thisSampeTargetPath = aux_SampeTargetPath\n",
    "        thisHistAvail = aux_HistAvail\n",
    "        thisTargetAvail = aux_TargetAvail\n",
    "        TimeStamp = aux_TimeStamp\n",
    "        thisTrackID = aux_TrackID\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # Predict\n",
    "    if base_model:\n",
    "        predPath = forwardpass_use(thisSampleMapComp, ImageEncModel,  PathDecModel)\n",
    "    else:\n",
    "        PathDecModel.reset_states()\n",
    "        HistEncModel.reset_states()\n",
    "        predPath = forwardpass_use(thisSampleMapComp, thisSampeHistPath, thisHistAvail, 50,\n",
    "                                                ImageEncModel, HistEncModel, PathDecModel,\n",
    "                                                use_teacher_force=False,\n",
    "                                                increment_net = increment_net,\n",
    "                                                mruv_guiding = mruv_guiding,\n",
    "                                                mruv_model = mruv_model,\n",
    "                                                mruv_model_trainable = mruv_model_trainable)\n",
    "    \n",
    "    predPath = predPath.numpy()\n",
    "\n",
    "    # convert agent coordinates into world offsets\n",
    "    predPath = predPath[:,:,:2]\n",
    "    world_from_agents = thisWorldFromAgent.numpy()\n",
    "    centroids = thisCentroid.numpy()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for idx_sample in range(gen_batch_size-pad_size):\n",
    "        \n",
    "        # Save info\n",
    "        future_coords_offsets_pd.append(transform_points(predPath[idx_sample,:,:], thisWorldFromAgent.numpy()[idx_sample,:,:]) -thisCentroid.numpy()[idx_sample,:])\n",
    "        timestamps.append(thisTimeStamp[idx_sample].numpy())\n",
    "        agent_ids.append(thisTrackID[idx_sample])\n",
    "\n",
    "assert len(agent_ids) == len(validation_dataset), \"Validation data size not equal to dataset.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = '/tf/2020-10-Lyft/prediction-dataset/validate_chopped_100/validation_submission.csv'\n",
    "\n",
    "write_pred_csv(pred_path,\n",
    "               timestamps=np.array(timestamps),\n",
    "               track_ids=np.array(agent_ids, dtype=np.int32),\n",
    "               coords=np.array(future_coords_offsets_pd),\n",
    "               confs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "metricas_out = compute_metrics_csv(eval_gt_path, pred_path, [neg_multi_log_likelihood, \n",
    "                                                             time_displace, \n",
    "                                                             rmse, \n",
    "                                                             average_displacement_error_mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "neg_multi_log_likelihood 244.92919855052006\ntime_displace [0.04934764 0.08579769 0.12089393 0.1561413  0.19210552 0.228615\n 0.26647733 0.30567488 0.34602303 0.38794152 0.43244486 0.47885622\n 0.52605959 0.5742509  0.62246936 0.67125626 0.71984946 0.76881484\n 0.81636672 0.86517557 0.91392045 0.96253291 1.01190729 1.05998885\n 1.1075651  1.15481851 1.20301443 1.24916558 1.2981305  1.34444181\n 1.39212355 1.43916194 1.48821257 1.53370729 1.58073594 1.62679886\n 1.67275219 1.71962976 1.76466566 1.81135897 1.8579979  1.90341346\n 1.95021814 1.99750129 2.04603132 2.09103832 2.13543204 2.17920677\n 2.22588173 2.27277471]\nrmse 1.4403223951224309\naverage_displacement_error_mean 5.36015060563473\n"
     ]
    }
   ],
   "source": [
    "for metric_name, metric_mean in metricas_out.items():\n",
    "    print(metric_name, metric_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}